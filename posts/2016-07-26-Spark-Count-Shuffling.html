<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>My Hakyll Blog - Apache Spark - Does Shuffling Occur When Count Actions are Performed?</title>
        <link rel="stylesheet" type="text/css" href="../css/default.css" />
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="../">My Hakyll Blog</a>
            </div>
            <div id="navigation">
                <a href="../">Home</a>
                <a href="../about.html">About</a>
                <a href="../contact.html">Contact</a>
                <a href="../archive.html">Archive</a>
            </div>
        </div>

        <div id="content">
            <h1>Apache Spark - Does Shuffling Occur When Count Actions are Performed?</h1>

            <div class="info">
    Posted on July 26, 2016
    
</div>

<p>While <a href="http://spark.apache.org/">Apache Spark</a>’s level of abstraction eases the development of jobs running on distributed data, it’s not always easy to figure out how to optimize them, or how to avoid common pitfalls. A well-known source of performance issues is <a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-rdd-shuffle.html">shuffling</a>. Shuffling is a process of data redistribution across partitions; when you deal with a huge amount of data (the infamous Big Data, you know) and these data can be moved over the wire, shuffling may take a considerable amount of time. While the decision of avoiding shuffling as much as possible is a no-brainer, it isn’t always easy to figure out which operations may cause shuffling.</p>
<p>In this blog post, I’ll prove that shuffling doesn’t occur when count() is invoked.</p>
<h3 id="prerequisites">Prerequisites</h3>
<p>I’ll use <a href="https://spark.apache.org/news/spark-1-6-1-released.html">Apache Spark 1.6.1</a>. At the time of writing, it’s the latest stable release available.</p>
<h3 id="apache-spark-job-example">Apache Spark Job Example</h3>
<p>As you may very well know, count() is an <a href="http://spark.apache.org/docs/latest/programming-guide.html#actions">action</a>; in the Spark lingo, this means that count(): - is eagerly evaluated - doesn’t return another RDD</p>
<p>Moreover, for obvious reasons, count() needs to iterate the whole data set. All this may lead to the belief that count() leads to data shuffling. But does it really happen? Let’s try it.</p>
<p>I created a simple (dumb) Spark job:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode scala"><code class="sourceCode scala"><a class="sourceLine" id="cb1-1" title="1"><span class="kw">val</span> shakespeareRDD = sc.<span class="fu">textFile</span>(getClass.<span class="fu">getResource</span>(<span class="st">&quot;/all-shakespeare.txt&quot;</span>).<span class="fu">getPath</span>)</a>
<a class="sourceLine" id="cb1-2" title="2">  .<span class="fu">flatMap</span>(_.<span class="fu">split</span>(<span class="st">&quot;&quot;&quot;\w+&quot;&quot;&quot;</span>))</a>
<a class="sourceLine" id="cb1-3" title="3"></a>
<a class="sourceLine" id="cb1-4" title="4"><span class="kw">val</span> wordCountRDD = shakespeareRDD.<span class="fu">count</span></a>
<a class="sourceLine" id="cb1-5" title="5">logger.<span class="fu">info</span>(s<span class="st">&quot;There are $wordCountRDD words contained in the documents. Using an RDD.&quot;</span>)</a></code></pre></div>
<p>Let’s analyze what happens here: a text file is loaded from the resource folder, then each line is turned into a bag of words, and finally the words are counted. Simple as that. Note that a word can be counted more than once; this means that, in case of a phrase like “Hello World Hello”, the result will be 3 (i.e. “Hello” will be counted twice) instead of 2.</p>
<p>Now, does shuffling occur? Let’s open Spark’s <a href="http://spark.apache.org/docs/latest/monitoring.html#web-interfaces">Web UI</a>. From here, let’s click on the “Stages” tab and select the “count at WordCount.scala” stage. Finally, let’s open the Event Timeline.</p>
<figure>
<img src="%7B%7B%20site.url%20%7D%7D/assets/img/2016-07-26/event-timeline.png" alt="Spark Event Timeline" /><figcaption>Spark Event Timeline</figcaption>
</figure>
<p>Durations may vary a lot here, executor computing time may increase or decrease, but no shuffling write time was spent.</p>
<h3 id="why">Why?</h3>
<p>In order to understand why no shuffling occurs, let’s have a look at the source code. As stated above, this code refers to Apache Spark 1.6.1. Here’s the RDD implementation of method <a href="https://github.com/apache/spark/blob/branch-1.6/core/src/main/scala/org/apache/spark/rdd/RDD.scala">count()</a>:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode scala"><code class="sourceCode scala"><a class="sourceLine" id="cb2-1" title="1"><span class="co">/**</span></a>
<a class="sourceLine" id="cb2-2" title="2"> <span class="co">*</span> Return the number of elements in the RDD<span class="co">.</span></a>
<a class="sourceLine" id="cb2-3" title="3"> <span class="co">*/</span></a>
<a class="sourceLine" id="cb2-4" title="4"><span class="kw">def</span> <span class="fu">count</span>(): Long = sc.<span class="fu">runJob</span>(<span class="kw">this</span>, Utils.<span class="fu">getIteratorSize</span> _).<span class="fu">sum</span></a></code></pre></div>
<p>It seems here that most of the work is demanded to the Spark Context. Let’s see what runJob() is:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode scala"><code class="sourceCode scala"><a class="sourceLine" id="cb3-1" title="1"><span class="co">/**</span></a>
<a class="sourceLine" id="cb3-2" title="2"> <span class="co">*</span> Run a job on all partitions in an RDD and return the results in an array<span class="co">.</span></a>
<a class="sourceLine" id="cb3-3" title="3"> <span class="co">*/</span></a>
<a class="sourceLine" id="cb3-4" title="4"><span class="kw">def</span> runJob[T, U: ClassTag](rdd: RDD[T], func: Iterator[T] =&gt; U): Array[U] = {</a>
<a class="sourceLine" id="cb3-5" title="5">  <span class="fu">runJob</span>(rdd, func, <span class="dv">0</span> until rdd.<span class="fu">partitions</span>.<span class="fu">length</span>)</a>
<a class="sourceLine" id="cb3-6" title="6">}</a></code></pre></div>
<p>Let’s follow the invocation chain a bit.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode scala"><code class="sourceCode scala"><a class="sourceLine" id="cb4-1" title="1"><span class="co">/**</span></a>
<a class="sourceLine" id="cb4-2" title="2"> <span class="co">*</span> Run a function on a given set of partitions in an RDD and return the results as an array<span class="co">.</span></a>
<a class="sourceLine" id="cb4-3" title="3"> <span class="co">*/</span></a>
<a class="sourceLine" id="cb4-4" title="4"><span class="kw">def</span> runJob[T, U: ClassTag](</a>
<a class="sourceLine" id="cb4-5" title="5">    rdd: RDD[T],</a>
<a class="sourceLine" id="cb4-6" title="6">    func: (TaskContext, Iterator[T]) =&gt; U,</a>
<a class="sourceLine" id="cb4-7" title="7">    partitions: Seq[Int]): Array[U] = {</a>
<a class="sourceLine" id="cb4-8" title="8">  <span class="kw">val</span> results = <span class="kw">new</span> Array[U](partitions.<span class="fu">size</span>)</a>
<a class="sourceLine" id="cb4-9" title="9">  runJob[T, U](rdd, func, partitions, (index, res) =&gt; <span class="fu">results</span>(index) = res)</a>
<a class="sourceLine" id="cb4-10" title="10">  results</a>
<a class="sourceLine" id="cb4-11" title="11">}</a></code></pre></div>
<p>Seems here that an empty array (which will be the result of the computation) is initialized, then filled with some result. OK, everything is clear now, isn’t it? Well, not really. Let’s follow the invocation chain a little bit further, up to…</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode scala"><code class="sourceCode scala"><a class="sourceLine" id="cb5-1" title="1"><span class="co">/**</span></a>
<a class="sourceLine" id="cb5-2" title="2"> <span class="co">*</span> Run a function on a given set of partitions in an RDD and pass the results to the given</a>
<a class="sourceLine" id="cb5-3" title="3"> <span class="co">*</span> handler function<span class="co">. </span>This is the main entry point for all actions in Spark<span class="co">.</span></a>
<a class="sourceLine" id="cb5-4" title="4"> <span class="co">*/</span></a>
<a class="sourceLine" id="cb5-5" title="5"><span class="kw">def</span> runJob[T, U: ClassTag](</a>
<a class="sourceLine" id="cb5-6" title="6">    rdd: RDD[T],</a>
<a class="sourceLine" id="cb5-7" title="7">    func: (TaskContext, Iterator[T]) =&gt; U,</a>
<a class="sourceLine" id="cb5-8" title="8">    partitions: Seq[Int],</a>
<a class="sourceLine" id="cb5-9" title="9">    resultHandler: (Int, U) =&gt; Unit): Unit = {</a>
<a class="sourceLine" id="cb5-10" title="10">  <span class="kw">if</span> (stopped.<span class="fu">get</span>()) {</a>
<a class="sourceLine" id="cb5-11" title="11">    <span class="kw">throw</span> <span class="kw">new</span> IllegalStateException(<span class="st">&quot;SparkContext has been shutdown&quot;</span>)</a>
<a class="sourceLine" id="cb5-12" title="12">  }</a>
<a class="sourceLine" id="cb5-13" title="13">  <span class="kw">val</span> callSite = getCallSite</a>
<a class="sourceLine" id="cb5-14" title="14">  <span class="kw">val</span> cleanedFunc = <span class="fu">clean</span>(func)</a>
<a class="sourceLine" id="cb5-15" title="15">  <span class="fu">logInfo</span>(<span class="st">&quot;Starting job: &quot;</span> + callSite.<span class="fu">shortForm</span>)</a>
<a class="sourceLine" id="cb5-16" title="16">  <span class="kw">if</span> (conf.<span class="fu">getBoolean</span>(<span class="st">&quot;spark.logLineage&quot;</span>, <span class="kw">false</span>)) {</a>
<a class="sourceLine" id="cb5-17" title="17">    <span class="fu">logInfo</span>(<span class="st">&quot;RDD's recursive dependencies:</span><span class="ch">\n</span><span class="st">&quot;</span> + rdd.<span class="fu">toDebugString</span>)</a>
<a class="sourceLine" id="cb5-18" title="18">  }</a>
<a class="sourceLine" id="cb5-19" title="19">  dagScheduler.<span class="fu">runJob</span>(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.<span class="fu">get</span>)</a>
<a class="sourceLine" id="cb5-20" title="20">  progressBar.<span class="fu">foreach</span>(_.<span class="fu">finishAll</span>())</a>
<a class="sourceLine" id="cb5-21" title="21">  rdd.<span class="fu">doCheckpoint</span>()</a>
<a class="sourceLine" id="cb5-22" title="22">}</a></code></pre></div>
<p>OK, this is the final runJob() method, but (surprise!) it calls yet another runJob() method, this time the DAG Scheduler’s.</p>
<p>Let’s have a look at this one, too.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode scala"><code class="sourceCode scala"><a class="sourceLine" id="cb6-1" title="1"><span class="co">/**</span></a>
<a class="sourceLine" id="cb6-2" title="2"> <span class="co">*</span> Run an action job on the given RDD and pass all the results to the resultHandler function as</a>
<a class="sourceLine" id="cb6-3" title="3"> <span class="co">*</span> they arrive<span class="co">.</span></a>
<a class="sourceLine" id="cb6-4" title="4"> <span class="co">*</span></a>
<a class="sourceLine" id="cb6-5" title="5"> <span class="co">*</span> <span class="an">@param rdd </span>target RDD to run tasks on</a>
<a class="sourceLine" id="cb6-6" title="6"> <span class="co">*</span> <span class="an">@param func </span>a function to run on each partition of the RDD</a>
<a class="sourceLine" id="cb6-7" title="7"> <span class="co">*</span> <span class="an">@param partitions </span>set of partitions to run on<span class="co">;</span> some jobs may not want to compute on all</a>
<a class="sourceLine" id="cb6-8" title="8"> <span class="co">*</span>   partitions of the target RDD<span class="co">,</span> e<span class="co">.</span>g<span class="co">.</span> for operations like first<span class="co">()</span></a>
<a class="sourceLine" id="cb6-9" title="9"> <span class="co">*</span> <span class="an">@param callSite </span>where in the user program this job was called</a>
<a class="sourceLine" id="cb6-10" title="10"> <span class="co">*</span> <span class="an">@param resultHandler </span>callback to pass each result to</a>
<a class="sourceLine" id="cb6-11" title="11"> <span class="co">*</span> <span class="an">@param properties </span>scheduler properties to attach to this job<span class="co">,</span> e<span class="co">.</span>g<span class="co">.</span> fair scheduler pool name</a>
<a class="sourceLine" id="cb6-12" title="12"> <span class="co">*</span></a>
<a class="sourceLine" id="cb6-13" title="13"> <span class="co">*</span> <span class="an">@throws Exception </span>when the job fails</a>
<a class="sourceLine" id="cb6-14" title="14"> <span class="co">*/</span></a>
<a class="sourceLine" id="cb6-15" title="15"><span class="kw">def</span> runJob[T, U](</a>
<a class="sourceLine" id="cb6-16" title="16">    rdd: RDD[T],</a>
<a class="sourceLine" id="cb6-17" title="17">    func: (TaskContext, Iterator[T]) =&gt; U,</a>
<a class="sourceLine" id="cb6-18" title="18">    partitions: Seq[Int],</a>
<a class="sourceLine" id="cb6-19" title="19">    callSite: CallSite,</a>
<a class="sourceLine" id="cb6-20" title="20">    resultHandler: (Int, U) =&gt; Unit,</a>
<a class="sourceLine" id="cb6-21" title="21">    properties: Properties): Unit = {</a>
<a class="sourceLine" id="cb6-22" title="22">  <span class="kw">val</span> start = System.<span class="fu">nanoTime</span></a>
<a class="sourceLine" id="cb6-23" title="23">  <span class="kw">val</span> waiter = <span class="fu">submitJob</span>(rdd, func, partitions, callSite, resultHandler, properties)</a>
<a class="sourceLine" id="cb6-24" title="24">  <span class="co">// Note: Do not call Await.ready(future) because that calls `scala.concurrent.blocking`,</span></a>
<a class="sourceLine" id="cb6-25" title="25">  <span class="co">// which causes concurrent SQL executions to fail if a fork-join pool is used. Note that</span></a>
<a class="sourceLine" id="cb6-26" title="26">  <span class="co">// due to idiosyncrasies in Scala, `awaitPermission` is not actually used anywhere so it's</span></a>
<a class="sourceLine" id="cb6-27" title="27">  <span class="co">// safe to pass in null here. For more detail, see SPARK-13747.</span></a>
<a class="sourceLine" id="cb6-28" title="28">  <span class="kw">val</span> awaitPermission = <span class="kw">null</span>.<span class="fu">asInstanceOf</span>[scala.<span class="fu">concurrent</span>.<span class="fu">CanAwait</span>]</a>
<a class="sourceLine" id="cb6-29" title="29">  waiter.<span class="fu">completionFuture</span>.<span class="fu">ready</span>(Duration.<span class="fu">Inf</span>)(awaitPermission)</a>
<a class="sourceLine" id="cb6-30" title="30">  waiter.<span class="fu">completionFuture</span>.<span class="fu">value</span>.<span class="fu">get</span> <span class="kw">match</span> {</a>
<a class="sourceLine" id="cb6-31" title="31">    <span class="kw">case</span> scala.<span class="fu">util</span>.<span class="fu">Success</span>(_) =&gt;</a>
<a class="sourceLine" id="cb6-32" title="32">      <span class="fu">logInfo</span>(<span class="st">&quot;Job %d finished: %s, took %f s&quot;</span><span class="fu">.format</span></a>
<a class="sourceLine" id="cb6-33" title="33">        (waiter.<span class="fu">jobId</span>, callSite.<span class="fu">shortForm</span>, (System.<span class="fu">nanoTime</span> - start) / <span class="fl">1e9</span>))</a>
<a class="sourceLine" id="cb6-34" title="34">    <span class="kw">case</span> scala.<span class="fu">util</span>.<span class="fu">Failure</span>(exception) =&gt;</a>
<a class="sourceLine" id="cb6-35" title="35">      <span class="fu">logInfo</span>(<span class="st">&quot;Job %d failed: %s, took %f s&quot;</span><span class="fu">.format</span></a>
<a class="sourceLine" id="cb6-36" title="36">        (waiter.<span class="fu">jobId</span>, callSite.<span class="fu">shortForm</span>, (System.<span class="fu">nanoTime</span> - start) / <span class="fl">1e9</span>))</a>
<a class="sourceLine" id="cb6-37" title="37">      <span class="co">// SPARK-8644: Include user stack trace in exceptions coming from DAGScheduler.</span></a>
<a class="sourceLine" id="cb6-38" title="38">      <span class="kw">val</span> callerStackTrace = Thread.<span class="fu">currentThread</span>().<span class="fu">getStackTrace</span>.<span class="fu">tail</span></a>
<a class="sourceLine" id="cb6-39" title="39">      exception.<span class="fu">setStackTrace</span>(exception.<span class="fu">getStackTrace</span> ++ callerStackTrace)</a>
<a class="sourceLine" id="cb6-40" title="40">      <span class="kw">throw</span> exception</a>
<a class="sourceLine" id="cb6-41" title="41">  }</a>
<a class="sourceLine" id="cb6-42" title="42">}</a></code></pre></div>
<p>Almost there now. It seems that what we are looking for is contained inside method submitJob().</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode scala"><code class="sourceCode scala"><a class="sourceLine" id="cb7-1" title="1"><span class="co">/**</span></a>
<a class="sourceLine" id="cb7-2" title="2"> <span class="co">*</span> Submit an action job to the scheduler<span class="co">.</span></a>
<a class="sourceLine" id="cb7-3" title="3"> <span class="co">*</span></a>
<a class="sourceLine" id="cb7-4" title="4"> <span class="co">*</span> <span class="an">@param rdd </span>target RDD to run tasks on</a>
<a class="sourceLine" id="cb7-5" title="5"> <span class="co">*</span> <span class="an">@param func </span>a function to run on each partition of the RDD</a>
<a class="sourceLine" id="cb7-6" title="6"> <span class="co">*</span> <span class="an">@param partitions </span>set of partitions to run on<span class="co">;</span> some jobs may not want to compute on all</a>
<a class="sourceLine" id="cb7-7" title="7"> <span class="co">*</span>   partitions of the target RDD<span class="co">,</span> e<span class="co">.</span>g<span class="co">.</span> for operations like first<span class="co">()</span></a>
<a class="sourceLine" id="cb7-8" title="8"> <span class="co">*</span> <span class="an">@param callSite </span>where in the user program this job was called</a>
<a class="sourceLine" id="cb7-9" title="9"> <span class="co">*</span> <span class="an">@param resultHandler </span>callback to pass each result to</a>
<a class="sourceLine" id="cb7-10" title="10"> <span class="co">*</span> <span class="an">@param properties </span>scheduler properties to attach to this job<span class="co">,</span> e<span class="co">.</span>g<span class="co">.</span> fair scheduler pool name</a>
<a class="sourceLine" id="cb7-11" title="11"> <span class="co">*</span></a>
<a class="sourceLine" id="cb7-12" title="12"> <span class="co">*</span> <span class="an">@return </span>a JobWaiter object that can be used to block until the job finishes executing</a>
<a class="sourceLine" id="cb7-13" title="13"> <span class="co">*</span>         or can be used to cancel the job<span class="co">.</span></a>
<a class="sourceLine" id="cb7-14" title="14"> <span class="co">*</span></a>
<a class="sourceLine" id="cb7-15" title="15"> <span class="co">*</span> <span class="an">@throws IllegalArgumentException </span>when partitions ids are illegal</a>
<a class="sourceLine" id="cb7-16" title="16"> <span class="co">*/</span></a>
<a class="sourceLine" id="cb7-17" title="17"><span class="kw">def</span> submitJob[T, U](</a>
<a class="sourceLine" id="cb7-18" title="18">    rdd: RDD[T],</a>
<a class="sourceLine" id="cb7-19" title="19">    func: (TaskContext, Iterator[T]) =&gt; U,</a>
<a class="sourceLine" id="cb7-20" title="20">    partitions: Seq[Int],</a>
<a class="sourceLine" id="cb7-21" title="21">    callSite: CallSite,</a>
<a class="sourceLine" id="cb7-22" title="22">    resultHandler: (Int, U) =&gt; Unit,</a>
<a class="sourceLine" id="cb7-23" title="23">    properties: Properties): JobWaiter[U] = {</a>
<a class="sourceLine" id="cb7-24" title="24">  <span class="co">// Check to make sure we are not launching a task on a partition that does not exist.</span></a>
<a class="sourceLine" id="cb7-25" title="25">  <span class="kw">val</span> maxPartitions = rdd.<span class="fu">partitions</span>.<span class="fu">length</span></a>
<a class="sourceLine" id="cb7-26" title="26">  partitions.<span class="fu">find</span>(p =&gt; p &gt;= maxPartitions || p &lt; <span class="dv">0</span>).<span class="fu">foreach</span> { p =&gt;</a>
<a class="sourceLine" id="cb7-27" title="27">    <span class="kw">throw</span> <span class="kw">new</span> IllegalArgumentException(</a>
<a class="sourceLine" id="cb7-28" title="28">      <span class="st">&quot;Attempting to access a non-existent partition: &quot;</span> + p + <span class="st">&quot;. &quot;</span> +</a>
<a class="sourceLine" id="cb7-29" title="29">        <span class="st">&quot;Total number of partitions: &quot;</span> + maxPartitions)</a>
<a class="sourceLine" id="cb7-30" title="30">  }</a>
<a class="sourceLine" id="cb7-31" title="31"></a>
<a class="sourceLine" id="cb7-32" title="32">  <span class="kw">val</span> jobId = nextJobId.<span class="fu">getAndIncrement</span>()</a>
<a class="sourceLine" id="cb7-33" title="33">  <span class="kw">if</span> (partitions.<span class="fu">size</span> == <span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb7-34" title="34">    <span class="co">// Return immediately if the job is running 0 tasks</span></a>
<a class="sourceLine" id="cb7-35" title="35">    <span class="kw">return</span> <span class="kw">new</span> JobWaiter[U](<span class="kw">this</span>, jobId, <span class="dv">0</span>, resultHandler)</a>
<a class="sourceLine" id="cb7-36" title="36">  }</a>
<a class="sourceLine" id="cb7-37" title="37"></a>
<a class="sourceLine" id="cb7-38" title="38">  <span class="fu">assert</span>(partitions.<span class="fu">size</span> &gt; <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb7-39" title="39">  <span class="kw">val</span> func2 = func.<span class="fu">asInstanceOf</span>[(TaskContext, Iterator[_]) =&gt; _]</a>
<a class="sourceLine" id="cb7-40" title="40">  <span class="kw">val</span> waiter = <span class="kw">new</span> <span class="fu">JobWaiter</span>(<span class="kw">this</span>, jobId, partitions.<span class="fu">size</span>, resultHandler)</a>
<a class="sourceLine" id="cb7-41" title="41">  eventProcessLoop.<span class="fu">post</span>(<span class="fu">JobSubmitted</span>(</a>
<a class="sourceLine" id="cb7-42" title="42">    jobId, rdd, func2, partitions.<span class="fu">toArray</span>, callSite, waiter,</a>
<a class="sourceLine" id="cb7-43" title="43">    SerializationUtils.<span class="fu">clone</span>(properties)))</a>
<a class="sourceLine" id="cb7-44" title="44">  waiter</a>
<a class="sourceLine" id="cb7-45" title="45">}</a></code></pre></div>
<p>What does JobWaiter do? While it is quite apparent, let’s just focus on one of its methods: taskSucceeded(), which will use the resultHandler function, the last piece of the puzzle.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode scala"><code class="sourceCode scala"><a class="sourceLine" id="cb8-1" title="1"><span class="kw">override</span> <span class="kw">def</span> <span class="fu">taskSucceeded</span>(index: Int, result: Any): Unit = {</a>
<a class="sourceLine" id="cb8-2" title="2">  <span class="co">// resultHandler call must be synchronized in case resultHandler itself is not thread safe.</span></a>
<a class="sourceLine" id="cb8-3" title="3">  synchronized {</a>
<a class="sourceLine" id="cb8-4" title="4">    <span class="fu">resultHandler</span>(index, result.<span class="fu">asInstanceOf</span>[T])</a>
<a class="sourceLine" id="cb8-5" title="5">  }</a>
<a class="sourceLine" id="cb8-6" title="6">  <span class="kw">if</span> (finishedTasks.<span class="fu">incrementAndGet</span>() == totalTasks) {</a>
<a class="sourceLine" id="cb8-7" title="7">    jobPromise.<span class="fu">success</span>(())</a>
<a class="sourceLine" id="cb8-8" title="8">  }</a>
<a class="sourceLine" id="cb8-9" title="9">}</a></code></pre></div>
<p>We now have everything we need: in fact, submitJob() tells us exactly what is going on under the hood (and probably you have already guessed, haven’t you?). It tells us that “func [is] a function to run <em>on each partition of the RDD</em>”. Similarly, method taskSucceeded() tells us what happens when the task… well… succeeds: the resultHandler function is invoked, applying the result and an index.</p>
<p>Let’s try to reconstruct what’s going on here: - count() calls method runJob() of SparkContext; - runJob() creates an empty “result” Array and (after a long invocation chain) calls method submitJob() of DAGScheduler - method submitJob() of DAGScheduler creates a JobWaiter object, which waits for a DAGScheduler job to complete. As soon as the task finishes, it passes its result to the given handler function.</p>
<p>What was this handler function? Well, in our case, we have to go back where everything started: method count(). There, we find out that the function that was passed was Utils.getIteratorSize(). Let’s have a look at it:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode scala"><code class="sourceCode scala"><a class="sourceLine" id="cb9-1" title="1"><span class="co">/**</span></a>
<a class="sourceLine" id="cb9-2" title="2"> <span class="co">*</span> Counts the number of elements of an iterator using a while loop rather than calling</a>
<a class="sourceLine" id="cb9-3" title="3"> <span class="co">*</span> <span class="co">[[</span>scala<span class="co">.</span>collection<span class="co">.</span>Iterator<span class="co">#</span>size<span class="co">]]</span> because it uses a for loop<span class="co">,</span> which is slightly slower</a>
<a class="sourceLine" id="cb9-4" title="4"> <span class="co">*</span> in the current version of Scala<span class="co">.</span></a>
<a class="sourceLine" id="cb9-5" title="5"> <span class="co">*/</span></a>
<a class="sourceLine" id="cb9-6" title="6"><span class="kw">def</span> getIteratorSize[T](iterator: Iterator[T]): Long = {</a>
<a class="sourceLine" id="cb9-7" title="7">  <span class="kw">var</span> count = 0L</a>
<a class="sourceLine" id="cb9-8" title="8">  <span class="kw">while</span> (iterator.<span class="fu">hasNext</span>) {</a>
<a class="sourceLine" id="cb9-9" title="9">    count += 1L</a>
<a class="sourceLine" id="cb9-10" title="10">    iterator.<span class="fu">next</span>()</a>
<a class="sourceLine" id="cb9-11" title="11">  }</a>
<a class="sourceLine" id="cb9-12" title="12">  count</a>
<a class="sourceLine" id="cb9-13" title="13">}</a></code></pre></div>
<p>Just a simple counter! So, our “result” Array (you remember it, don’t you) is simply filled with the count of the elements in each partition.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode scala"><code class="sourceCode scala"><a class="sourceLine" id="cb10-1" title="1"><span class="co">/**</span></a>
<a class="sourceLine" id="cb10-2" title="2"> <span class="co">*</span> Return the number of elements in the RDD<span class="co">.</span></a>
<a class="sourceLine" id="cb10-3" title="3"> <span class="co">*/</span></a>
<a class="sourceLine" id="cb10-4" title="4"><span class="kw">def</span> <span class="fu">count</span>(): Long = sc.<span class="fu">runJob</span>(<span class="kw">this</span>, Utils.<span class="fu">getIteratorSize</span> _).<span class="fu">sum</span></a></code></pre></div>
<p>After that, we get the sum of the Array and (ta-daan!) here is our count.</p>
<h3 id="conclusion">Conclusion</h3>
<p>So, how could that happen? Why wasn’t it obvious since the beginning?</p>
<p>Well, there’s no easy answer here, no fancy source we can browse. If I may put my two cents in, I would say that many developers (including me) tend to mistake shuffling for simple data movement. While data movement simply means… well… moving some kind of data, shuffling is a bit more tricky: it doesn’t simply move data; it moves data to the right partition.</p>
<p>Consider your typical Hadoop MapReduce job: there, shuffling isn’t just moving data; it’s moving all data pertaining to a certain word to the same reducer. In other words, the system must send the data of a certain word to <em>all</em> reducers (i.e. to all nodes of the cluster), so that data is correctly aggregated; this has two important effects: - data is moved from one node to all the others, so the number of movements is exponential; - data is quite raw, since it will be aggregated after the shuffling phase.</p>
<p>On the other hand, our simple count example, here, doesn’t require shuffling: data is moved from a node to the master (only one move), and data is almost completely refined (each partition returns the number of items it contains, so just an integer is moved over the wire).</p>
<p>And that’s it! Hope it helps!</p>

        </div>
        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
